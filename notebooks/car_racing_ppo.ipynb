{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Racing with PPO - Reinforcement Learning Project\n",
    "\n",
    "This notebook demonstrates the implementation and training of a **Proximal Policy Optimization (PPO)** agent for the **Car Racing** environment.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Environment Setup](#environment)\n",
    "3. [PPO Algorithm](#ppo)\n",
    "4. [Training](#training)\n",
    "5. [Evaluation](#evaluation)\n",
    "6. [Results Analysis](#results)\n",
    "7. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "### Problem Overview\n",
    "\n",
    "**Car Racing** is a continuous control task where an agent must learn to drive a car around a racing track. The challenge involves:\n",
    "- **Continuous action space**: steering, acceleration, and braking\n",
    "- **Pixel-based observations**: 96x96 RGB images\n",
    "- **Sparse rewards**: +1000/N for visiting track tiles, negative rewards for going off-track\n",
    "\n",
    "### Why PPO?\n",
    "\n",
    "PPO is an excellent choice for this task because:\n",
    "- **Stable**: Clips policy updates to prevent destructive changes\n",
    "- **Sample efficient**: Uses GAE for better advantage estimation\n",
    "- **Continuous control**: Naturally handles continuous action spaces\n",
    "- **State-of-the-art**: One of the most successful modern RL algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from config.ppo_config import PPOConfig\n",
    "from src.environment import CarRacingEnv, NormalizeActions\n",
    "from src.ppo_agent import PPOAgent\n",
    "from src.utils import set_seed, evaluate_agent\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Setup <a name=\"environment\"></a>\n",
    "\n",
    "### Environment Preprocessing\n",
    "\n",
    "We apply several preprocessing steps:\n",
    "1. **Grayscale conversion**: Reduce from RGB to grayscale\n",
    "2. **Frame cropping**: Remove car dashboard (bottom part)\n",
    "3. **Resizing**: 96x96 → 84x84\n",
    "4. **Normalization**: Pixel values to [0, 1]\n",
    "5. **Frame stacking**: Stack 4 consecutive frames for temporal information\n",
    "6. **Frame skipping**: Repeat actions for 2 frames (action repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = NormalizeActions(CarRacingEnv())\n",
    "\n",
    "print(f\"Observation space: {env.observation_space.shape}\")\n",
    "print(f\"Action space: {env.action_space.shape}\")\n",
    "print(f\"Action space bounds: {env.action_space.low} to {env.action_space.high}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize environment\n",
    "obs, _ = env.reset(seed=42)\n",
    "\n",
    "# Show stacked frames\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 3))\n",
    "for i in range(4):\n",
    "    axes[i].imshow(obs[i], cmap='gray')\n",
    "    axes[i].set_title(f'Frame {i+1}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Stacked Frames (Grayscale)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO Algorithm <a name=\"ppo\"></a>\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "PPO optimizes a clipped surrogate objective:\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "where:\n",
    "- $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ is the probability ratio\n",
    "- $\\hat{A}_t$ is the advantage estimate (computed using GAE)\n",
    "- $\\epsilon$ is the clipping parameter (typically 0.2)\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "**Shared CNN Feature Extractor:**\n",
    "- Conv2D(4, 32, 8x8, stride=4) + ReLU\n",
    "- Conv2D(32, 64, 4x4, stride=2) + ReLU\n",
    "- Conv2D(64, 64, 3x3, stride=1) + ReLU\n",
    "- Flatten → 3136 features\n",
    "\n",
    "**Actor (Policy) Head:**\n",
    "- Linear(3136, 256) + ReLU\n",
    "- Linear(256, 256) + ReLU\n",
    "- Linear(256, 3) + Tanh\n",
    "- Outputs: mean actions + learnable log std\n",
    "\n",
    "**Critic (Value) Head:**\n",
    "- Linear(3136, 256) + ReLU\n",
    "- Linear(256, 256) + ReLU\n",
    "- Linear(256, 1)\n",
    "- Outputs: state value estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize PPO agent\n",
    "config = PPOConfig()\n",
    "set_seed(config.seed)\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "agent = PPOAgent(obs_shape, action_dim, config)\n",
    "\n",
    "print(\"PPO Agent initialized!\")\n",
    "print(f\"Device: {agent.device}\")\n",
    "print(f\"\\nNetwork architecture:\")\n",
    "print(agent.actor_critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Key hyperparameters for PPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameters\n",
    "hyperparams = {\n",
    "    'Learning Rate': config.learning_rate,\n",
    "    'Gamma (Discount)': config.gamma,\n",
    "    'GAE Lambda': config.gae_lambda,\n",
    "    'Clip Epsilon': config.clip_epsilon,\n",
    "    'Value Coefficient': config.value_coef,\n",
    "    'Entropy Coefficient': config.entropy_coef,\n",
    "    'Steps per Update': config.n_steps,\n",
    "    'Batch Size': config.batch_size,\n",
    "    'Epochs per Update': config.n_epochs,\n",
    "    'Max Grad Norm': config.max_grad_norm\n",
    "}\n",
    "\n",
    "for key, value in hyperparams.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training <a name=\"training\"></a>\n",
    "\n",
    "To train the agent, run the training script:\n",
    "\n",
    "```bash\n",
    "python train.py\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Train for 1,000,000 timesteps\n",
    "- Save checkpoints every 50 updates\n",
    "- Evaluate every 25 updates\n",
    "- Log training metrics\n",
    "- Save the best model based on evaluation performance\n",
    "\n",
    "### Training Progress\n",
    "\n",
    "Training typically takes 4-8 hours on a GPU (depending on hardware)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation <a name=\"evaluation\"></a>\n",
    "\n",
    "Load and evaluate a trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "checkpoint_path = \"../checkpoints/best_model.pt\"\n",
    "\n",
    "try:\n",
    "    agent.load(checkpoint_path)\n",
    "    print(f\"Model loaded from: {checkpoint_path}\")\n",
    "except:\n",
    "    print(\"No trained model found. Please train the model first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent\n",
    "eval_env = NormalizeActions(CarRacingEnv())\n",
    "mean_reward, std_reward, mean_length = evaluate_agent(agent, eval_env, n_episodes=10)\n",
    "\n",
    "print(f\"\\nEvaluation Results (10 episodes):\")\n",
    "print(f\"Mean Reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "print(f\"Mean Length: {mean_length:.1f}\")\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis <a name=\"results\"></a>\n",
    "\n",
    "Analyze training curves and performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training metrics\n",
    "try:\n",
    "    metrics = np.load(\"../logs/training_metrics.npz\")\n",
    "    \n",
    "    timesteps = metrics['timesteps']\n",
    "    episode_rewards = metrics['episode_rewards']\n",
    "    episode_lengths = metrics['episode_lengths']\n",
    "    policy_losses = metrics['policy_losses']\n",
    "    value_losses = metrics['value_losses']\n",
    "    \n",
    "    print(\"Training metrics loaded successfully!\")\n",
    "except:\n",
    "    print(\"No training metrics found. Train the model first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "def moving_average(data, window=10):\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window)/window, mode='valid')\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Episode rewards\n",
    "axes[0, 0].plot(timesteps, episode_rewards, alpha=0.3, label='Raw')\n",
    "axes[0, 0].plot(timesteps[:len(moving_average(episode_rewards, 50))], \n",
    "                moving_average(episode_rewards, 50), linewidth=2, label='MA(50)')\n",
    "axes[0, 0].set_xlabel('Timestep')\n",
    "axes[0, 0].set_ylabel('Episode Reward')\n",
    "axes[0, 0].set_title('Training Progress: Episode Rewards')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Episode lengths\n",
    "axes[0, 1].plot(timesteps, episode_lengths, alpha=0.3, label='Raw')\n",
    "axes[0, 1].plot(timesteps[:len(moving_average(episode_lengths, 50))], \n",
    "                moving_average(episode_lengths, 50), linewidth=2, label='MA(50)')\n",
    "axes[0, 1].set_xlabel('Timestep')\n",
    "axes[0, 1].set_ylabel('Episode Length')\n",
    "axes[0, 1].set_title('Training Progress: Episode Lengths')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Policy loss\n",
    "axes[1, 0].plot(policy_losses)\n",
    "axes[1, 0].set_xlabel('Update')\n",
    "axes[1, 0].set_ylabel('Policy Loss')\n",
    "axes[1, 0].set_title('Policy Loss over Updates')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Value loss\n",
    "axes[1, 1].plot(value_losses)\n",
    "axes[1, 1].set_xlabel('Update')\n",
    "axes[1, 1].set_ylabel('Value Loss')\n",
    "axes[1, 1].set_title('Value Loss over Updates')\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "print(\"Training Statistics:\")\n",
    "print(f\"Total Episodes: {len(episode_rewards)}\")\n",
    "print(f\"Final 100 Episodes - Mean Reward: {np.mean(episode_rewards[-100:]):.2f}\")\n",
    "print(f\"Final 100 Episodes - Mean Length: {np.mean(episode_lengths[-100:]):.1f}\")\n",
    "print(f\"Best Episode Reward: {max(episode_rewards):.2f}\")\n",
    "print(f\"Worst Episode Reward: {min(episode_rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion <a name=\"conclusion\"></a>\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this project, we successfully:\n",
    "1. Implemented a PPO agent from scratch using PyTorch\n",
    "2. Created a comprehensive environment wrapper with preprocessing\n",
    "3. Trained the agent on the Car Racing environment\n",
    "4. Achieved [YOUR RESULTS HERE] average reward\n",
    "5. Analyzed training dynamics and performance\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "- **Preprocessing is crucial**: Frame stacking and normalization significantly improve learning\n",
    "- **PPO is stable**: The clipped objective prevents destructive policy updates\n",
    "- **Hyperparameters matter**: GAE lambda and clip epsilon heavily influence convergence\n",
    "- **Sample efficiency**: PPO achieves good performance with reasonable sample complexity\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. **Architecture**: Try larger networks or attention mechanisms\n",
    "2. **Curriculum learning**: Start with simpler tracks\n",
    "3. **Data augmentation**: Random crops, color jitter for robustness\n",
    "4. **Reward shaping**: More sophisticated reward engineering\n",
    "5. **Ensemble methods**: Multiple agents for better exploration\n",
    "\n",
    "### References\n",
    "\n",
    "- Schulman et al. (2017). [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)\n",
    "- Schulman et al. (2016). [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n",
    "- [Gymnasium Documentation](https://gymnasium.farama.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
